- Moral _Agents_ & Moral _Objects_:
	- Children: objects but not agents (yet)
	- Future generations: unsure, definitely objects, perhaps agents
	- AI: not objects, perhaps agents
	- Climate: may be objects (why exactly?), no agent
	- There are border lines, adults are both subjects and object, children are only objects (you can do harm to them but shall have no responsibility, AI might be considered a subject but not an object. A stone is none of these.)
- _Axiologic/Evaluative_ terms (good, bad) vs _Deontic/Normative_ terms (should, right, wrong)
	- _Rawls_: utilitarianism begin with axiological, Kant with Deontic
- Descriptive (just describe a situation) or Normative (describe and judge)
- Three Macrotheories
	- Virtue Ethic (on subj.) {Aristotle}
	- Deontology (rules, way to conduct) {Kant}
	- Consequentialism (cons. of actions) {Utilitarianism}
		- Confusions: _Mill_: Aristotle is utilitarinanist, Kant is virtue ethics
- Need to pick one? No, you might go for the "common sense morality"
- Three Levels of Research: {on the trolley problem}
	- _Metaethics_: Semantical, Metaphysical and Epistemic questions
		- _Metaphysical_:
			- are moral properties metaphysical properties?
		- _Epistemical_:
			- is there moral knowledge? How do we get it?
		- _Psychological_ questions are sometimes also metaethical questions
			- what roles do emotions play? (cf. Hume)
		- {where do intuitions come from? Can we justify them rationally?}
		- is metaethics relevant for Normative/Applied Ethics?
		- _Kantian Theory_ is in the middle of meta and normative
			- what is a duty?
	- _Normative Ethics_: what does utilitarianism/kantianism/aristotelism say on this?
		- compare ethical systems and derive actions
	- _Applied Ethics_:
		- Clima
		- Animals (Bentham/Mill)
		- AI
		- Bioethics
### Consequentialism
- **Consequentialism** (Anscombe): morally judged are nothing but consequences of actions
	- value monism / value pluralism
	- Three most popular options:
		- Hedonistic theories: _pleasure_ (Bentham, $\sim$Mill), monist
		- Preference Utilitarianism: (D. Parfit, _Reasons and Persons_),
			- _actual_ preferences? _well informed_ ones? only the _moral_ ones?
		- List Theory: pluralist
			- there is a list things one should consider, e.g. friendship, love...
	- **Utilitarianism**: maximise utility/happyness (value monist)
		- Action-utilitarianism (judge actions) / Rules-utilitarnianism (judge rules)
			- each case is compatible with each of the following cases.
		- People yet to exist must be considered
		- **Bentham**
			- Bentham *Principle of Utility*: maximise utility
				- Utility is absence of pain
			- Bentham _Subjective Hedonistic Utility_
				- Alternatives: (i) objective theory of pleasure, (ii) satisfy wishes
			- Bentham _Animal's Utility is as valuable_
		- **Preference Utilitarianism**: the utility is to satisfy wishes (D. Parfit, _Reasons and Persons_)
			- _actual_ preferences? _well informed_ ones? only the _moral_ ones?
		- Arguments: **Pro** _utilitarianism_
			1. Bentham: we have both the intuition to do good for ourselves and for the community, we therefore need to find an objective criterion that quantifies that. Utilitarianism is a starting point, an intuition.
			 - C: people have different intuitions, hence it is not evident that there is a unique measure of common utility.
		- Bentham critiques to his time:
			- Animal rights, slaves, gender equality, jails, homosexuality
		- Against Bentham
			- _Philosophy of Swine_: utility cannot be just pleasure
		2. **Mill's Solution**: different _qualities_ of happiness (Kartoffeln essen << Art)
			- Plato: different levels, no amount of potatos makes a poem.
			- Mill: animals' pleasure is at a lower quality than humans', still mor.obj.
			- How to distinguish? Look at the educated class
				- still: utility is absence of pain
		- Arguments: **Contra** _utilitarianism_
			1. Utilitarianism is unpractical, how shall I compute it?
				- Mill: _no harm principle_: "you are free until you harm someone else".
			2. Freedom is good just bc it enables higher utility (for Kant it must be a first principle)
			3. _The demandingness objection_: utilitarianism requires us to be constantly active
				- Mill: _no harm principle_: not harming is "good enough".
				- Sidgwicks: just follow the common sense
			4. _Axiological Argument_: there's more than just pleasure to be maximised
				- switch to value pluralism consequentialism
	- Arguments: **Contra** _consequentialism_
		1. _moral saint_ (Wolfs): it's good to spend the life helping, would you marry someone like that?
		2. _Deontic argument_: other than utilities, there are rights and duties to respect.
			- Bentham; _natural rights are nonsense_ (a true English man)
			- If one could, should one destroy the plane on 9/11?
### Kantian Deontology
- it is a subset of all Deontology theories, one might argue against this though.
- three reasons to behave: (i) personal advantage {bad}, (ii) bc its just funny {bad}, (iii) from duty {yay}
	- moral law comes from pure practical reason, every subject with reason has it.
	- cf. Hume: morality arises from emotions
- imperatives: hypothetical (if that then) and categorical (no matters what, do that)
- every creature with reason has one principle in mind:
	- (i) "_handle nur nach derjenigen Maxime, durch die du zugleich wollen kannst, dass sie ein allgemeines Gesetz werde._"
	- (ii) "_handle so, als ob die Maxime deiner Handlung durch deinen Willen zum allgemeinen Naturgesetze werden sollte_"
	- Four classes of duties:
		- on yourself & absolute (/positive): preserve your life
		- on yourself & not absolute (/negative): develop your talents
		- on others & absolute (/positive): do not make false promises
		- on others & not absolute (/negative): make others happy
	- People shall be no means
		- (iii) "_der Mensch und überhaupt jedes vernünftige Wesen existiert als Zweck an sich selbst, nicht bloß als Mittel zum beliebigen Gebrauche für diesen oder jenen Wille_"
			- Moral objects are rational creatures (not animals, perhaps aliens)
			- Kant: (iii) $\leftrightarrow$ (ii) $\leftrightarrow$ (i)
	- Will is free (cf. Descartes)
		- positive free will is a good will, i.e. one that choses general principles to respect
		- _sollen $\to$ können_: I dont wanna do X, if I had to do X, then I know I am able to do X, despite not wanting to do X
	- _Reich der Zwecke_: dream world where people are all treated as ends.
	- Hegel: _leerer Formalismus_, Mill: counterexamples, Schiller: where are emotions?
### Virtue Ethic
- **Virtue**:
	- _Kant_: some moral fatigue
	- _Hume_: features of personality
		- is a virtue if it gives pleasure (or is useful) to me or others
- **Greece**: _Socrates_ is the idol for all
	- _Epicurus_: happiness, and virtue as a way to the piece of the soul
	- _Stoics_: pure virtue, control just because its right
	- _Sceptics_: look for ataraxia, dont give a shit
	- _Cynics_: independency from convention to get control on the life
- **Aristotle**: we need to start from pure knowledge
	- all that exists has a function, our is to reach εὐδαιμονία
		- men as _political_, _rational_ and _social_ creatures
		- _prefectionism_: we shall get the perfect state of our nature
	- _erstens_ all activities have a streben nach good a right ends
	- _zweitens_ eudaimonia is the highest end, the happiness
	- **Happiness**: εὐδαιμονία, Mill: that is the only utility, Aristotle is utilitarianist
		- "activity of the soul in accordance with the \[best\] virtue \[...\] and the most complete goal"
		- count the whole life, not a day. Only after death you can tell.
			- if kids die after our death, that also bad for us. 
		- not only absence of pain but happiness in doing human activities
		- without friends no one is happy, social virtues are impoerant too
		- not essential:
			- Lust & Freuden
	- **Virtue**: ἀρετή
		- "abitude of a good-working person" but also "feature that we admire in others"
		- require training (no enjoyment at the beginning)
		- good education is necessary
		- _in media stat virtus_
		- not all in our power, must be lucky
- Critiques:
	- Psychologically realistic? Cultural relativism? Egoism?
### Other Theories
- Rawls (see [[Political Philosophy (Lecture)]]): _Reflective Equilibrium_
	- a _meta-method_, it requires some other methods like _conceptual engineering_, _Aristotelian teleology_
	- how can I get a balance between the rational results from the method and the intuitions?
		- (i) general ethical values, (ii) mid-level principles, (iii) judgments on specific situations
	- Two Truth Theories
		- _Moral Realist_: two incoherent set of moral rules, one must be wrong (_D. Parfit_)
		- _Coherentism_: as long as the theory is coherent, then it's fine.
	- Sidgwick, moral theories as methods
	- Bioethical Principles: (i) autonomy, (ii) care of the patient, (iii) avoid damage, (iv) social justice
	- Other Theories: Constractualism, Feminism, (Confucianism, Ubuntu ethics)
### Responsibility
- _Shoemaker_: Faces of responsability: (i) attributibility, (ii) answeability, (iii) accountability
- _Kant_: Freedom is necessary or no behaviour shall be judged
	- Compatibilism: we are free, the world may be determinist or not (Strawson, Hume)
	- Incompatibilism: we are free $\to$ world is indeterminated ($\sim$Kant)
- _Frankfurt_: you are free when you are free to act on your wishes.
	- you have responsibility only if you could have done otherwise (_Frankfurt_)
- _Strawson_: responsibility is a human, not metaphysical matter; who cares about determinism
	- _participant stance_: treat other as responsible adults
	- _objective stance_: treat them as children that are not responsible
- Distinctions:
	- negative / positive and retrospective / prospective
		- prosp. pos, like dad and son (need to actively do stuff and for the future)
		- prosp. neg. you need to avoid some bad behaviour
			- similar things for those people that don't exist (yet) are harder to state
- Criteria: _control_, _foreseeability_, _understanding of the event_, under these circumstances one is resp.
	- _Williams_: we are almost always responsible
- Praise & Blame distinction
	- an active component is necessary for praise but not in blame
		- if you avoid to be good, you're blameworthy
		- praise shall not entail self-interest
		- blame can come with self-interest (and often does)
-  _CEO example_: moral evaluations influence perceptions of intentionality, (Joshua Knobe)
	- people are more likely to judge harmful actions as intentional than beneficial ones
	- Policies help/harm the environment, CEO knows it, he didn't/did it intentionally
- _Non-identity Problem_: (iii) we cannot change the future of ppl in bad scenarios (we'd change ppl)
	- (i) bad choices $\to$ sad future ppl, (ii) no bad choices $\to$ not those ppl
### Metaethics
- _Moral Realism_: (i) there r moral facts (metap.), (ii) those r true or false (sem.), (iii) we can know (epi.)
	- _Non-naturalism_: moral facts are not psych., phys. or in any way natural. Those they're out there
	- _Quietism_: need of a moral language/category, not reducible to truths of other sort
	- _Naturalism_
		- _Analytical Naturalism_: analytical (non natural), true beliefs
			- _open question argument_ which analytical principle shall we take? who knows?
			- e.g. $x$ is right $:=$ $x$ maximalises utility
- Motivational Internalism{ext.}: the motivation is {not} part of the moral belief (non-cogn.) {cogn.}
- _Direction of fit_: is the one of mor.jud. like the one of belief ($\to$non-cogn) or of desires ($\to$cogn) or both?
- _Anti Realism_: 
	- _Cognitivism_: moral judgements are beliefs
		- "_Error Theory_": moral beliefs are imagination of mor.truths, which actually don't exist (Joyce)
		- (_Judgement Dependent Theories_: those are true human opinions (Wright))
		- (_Cornell Realism_: those are true & more than human & natural & not nat.red. (Sturgeon))
		- (_Standard Moral Reductionism_: true& notjust.hum & nat. & not nat.red.)
	- _non-Cognitivism_: moral judgements are not beliefs, i.e. not true nor false
		- _Quasi-Realism_: there are moral truths, we just feel them but cannot phrase them (Blackburn)
			- (Blackburn): hence not subjective
			- (Ayer): non-cogn $\to$ mor.judg. depend on the subject
- Arguments Forms (Street):\
	- _vindicating_: explain the reason of moral judg., get justified belief
	- _debunking_: explain the reason of moral judg., and show it to be wrong
### Clima Ethics
- Does nature have a value on itself?
- Beautiful is well-formed, Sublime is power to compel and destroy
- _Leopold_: love, care, amazement are necessary for a good relationship with land
	- A behaviour is right if it protects integrity, stability and beauty of the biotical society
	- Ecofascism? individuals must not be meaningless, or just kill half of the population
		- _Calicott_: Clima ethics is just an extension
- _Rolston_: brang the issue in academia first
	- _practically_: protect the nature
	- _theoretically_: a _theory of values_ must be developed 
- _Parfit_: how can we satisfy our duties for future generations?
	- same/different people choices and sam/different number choices ==?==
	- average utilitarianism / sum utilitarianism
		- _repugnant conclusion_: large and poor poulation
	- again: _non identity problem_
- _Scheffer_: 
	- against the principle of well-behaviour  
	- if we were the last generation of mankind, how should we feel? is it important? yes bc:
		- Interest rule: it gives meaning and importance
		- Love rule: we love mankind
		- Value rule: value and meaning comes from future
		- Reciprocity rule: we use existence of future people, bc it makes our life meaningful
		- _Hence:_ future generations are even more important that our own!
- _Broom_: Divisions 
	- _Public Morality_ (duties of countries) vs _Private Morality_ (duties of individuals)
	- Duties of Charity vs Duties of Justice
		- _Charity_: nobody has the claim on our charity unless we have a special relation with them
		- _Justice_: toward those people that have the right not to be wronged.
	- _Private Morality_ on Climate Change the Duties of _Justice_ are more improtant
	- _Public Morality_ on Climate Change the Duties of _Charity_ are more important
	- Why our emissions are not right:
		1. The harm inflicted on future generations depends on the actions we take.
		2. The harm we cause is not trivial, but severe.
		3. The harm we cause is not accidental; we know we are causing it.
		4. We do not compensate the victims we harm.
		5. Most of our actions that harm future generations are for our own benefit.
		6. The harm caused by our emissions is not reciprocated.
		7. We could reduce our greenhouse gas emissions.
	- hence we should stop emittig or compensate
### AI Ethics
- Black Box, we know nothing of what happens between input e output
	- explainable AI tries to find out
- Is AI a _Moral Agent?_ is it a _Moral Object?_
- Three branches of the discussion:
	1. already present questions
	2. questions caused by misunderstandings by people
	3. on near and far future
- _Aristoteles_: AI and techne is cool!
- _Turing_: Turing test says that they can imitate
- _McCarthy_: same
- _Russel_: machines can perceive and behave
- Real Trolley Problem with AI, how should it solve it?
- _Moral Machine_: can it solve moral issues for us?
	- _Coeckelbergh_: no agents must have emotions
	- _Veliz_: no, moral agent must have coscience
	- couldn't AI have emotions and coscience?
- Machines and Power:
	- _Turing_: they'll overcome us
	- _Wiener_: the purpose in the machine must be the one we definitely desire
	- Solutions:
		- Limit the capacity (sad tho)
		- AI value Alignment
- Are we enslaving AI? if they get more developed?
- AI and _Responsibility_
	1. people take responsibility freely
	2. Perhaps we should welcome responsibility gaps
	3. AI may take responsibility instead of us
	4. People-Machines teamwork
- Interaction with people
	- Turing: we want it to be man-like, it may show us how we think
- can robots have ethical features?
- can they imitate them?
- can they represent them?